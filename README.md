# CAIS-Winter-Project-Muffin-vs.-Chihuahua-
1. Jonathan Ong
   ongjd@usc.edu

2. The aim of this project was to design a model that could classify images of chihuahuas and muffins. In particular, the goal was to use transfer learning from RESNET18 as a feature extractor to make creating and training the model faster and use less computing power.

3. I used the Muffin vs. Chihuahua dataset on Kaggle (https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification). For preprocessing, I resized the all images in the dataset to be 224x224 pixels and normalized all image values according to mean and standard deviation of the images on ImageNet. I chose to take these preprocessing steps because RESNET18 trained on ImageNet images which were all 224x224. So, I transformed the Muffin vs. Chihuahua dataset to match the images the RESNET18 trained on so that the feature extractors (upper layers of RESNET18) would still be valid/applicable. Some of the images in the dataset are grayscale, so I implemented logic to identify grayscale images, and update their tensors to be RGB.

4. For my model architecture, I used RESNET18 as the base. From there, I replaced the last fully connected layer so that it had two outputs (one for muffin and the other for chihuahua). I chose to have two outputs in the final layer instead of one with some sigmoid like function because muffins are not the opposite of chihuahuas. One output with a sigmoid like function would make it so for an image to be more like a muffin, it would have to be less like a chihuahua. For example, if I added blueberries to a picture with a chihuahua, it would be more likely to be a muffin. But, this doesn't make it any less of a chihuahua. Using two outputs in the final layer allows for measures of muffin-ness and chihuahua-ness to vary independently, and I am able to choose the highest value at the end to make a predication.
  Then, I made sure to freeze all layers, except for the fully connected layers. I did this both to speed up training and to preserve RESNET18's feature extractors (early layers). The dataset didn't have enough data to completely train the entire network, but by freezing the network, I was able to only train a small portion of it.
  For hyperparameters, I chose to have 5 epochs and a batch size of 16. I chose to have a batch size of 16 to speed up training (and I remember something about batch size always being a power of 2?). I chose to have 5 epochs to speed up training and hopefully avoid overfitting. Since I wasn't able to implement logic to track loss and stop training when loss stopped decreasing, my model was vulnerable to overfitting if I left it training for too long. So, I settled on a pretty low number of epochs to avoid this. This admittedly is not a perfect solution though, and it would be best to implement logic for early stopping when loss stops decreasing.
  For my loss function, I chose to use binary cross entropy because this was a classification problem between two classes of objects. For my optimizer, I decided to use Adam because it uses decaying momentum, which speeds of training and helps the model get out of local minimums (I also rememebred that Adam was one of the best optimizers from curriculum).

5. For my model evalutation I chose to use simple accuracy (# correct/ total). I chose to use simple accuracy (because of time limitations and not being able to figure out much else) and because simple accuracy weighs equally all types of error (false positives and false negatives). This seemed appropriate because in my opinion, classifying a chihuahua as a muffin and a muffin as a chihuahua seem equally inconsequential. My model after training had an accuracy of about 15% on the testing data. This extremely low accuracy had me very suspicious though since it is much worse than pure chance. So, I thought that I might have flipped my labelling in my testModel function. After I flipped my labelling, my model achieved an accuracy of 40%.

6. 
a) I think that the dataset, model architecture (RESNET18), and chosen metric fit the task of classifying muffins and chihuahuas pretty well. The dataset was pretty good at presenting different images of muffins and chihuahuas, but definitely could have been a bit better. For example, I noticed that some of the chihuahua pictures weren't really of chihuahuas (maybe just a shirt with a chihuahua's face on it). The RESNET18 model architecture as a base for transfer learning was a good choice for the task given the dataset and time/compute constraints. The training procedures could have been improved to better match the task. In particular, more steps could have been taken to prevent overfitting by implementing random transformations in image preprocessing and implementing early stopping in training. Using binary cross entropy as the loss function was well suited to the task because it involved classification between two classes.
b) Classification tasks always have wider implications because a lot of technology is based on being able to correctly classify objects (like self-driving cars). I think that this model is limited in that it used RESNET18 as a base for transfer learning. If this model were to be scaled up and used to contribute to social good, we would have to keep in mind that the model is based off of RESNET18. Thus, it would be likely for this model to display any biases that the RESNET18 model does. We would have to be very careful to deploy the model in an environment where these biases wouldn't come to play, or better yet not deploy the model at all in favor of developing a new one (that hopefully won't contain any RESNET18 shortcomings).
c) If I were continue this project, I would like to address some issues in its execution due to me not knowing how to do things or not having the time to figure out how to do things. One thing I would like to do is implement logic for early stopping once loss stops decreasing. This would allow me to confortably train the model for more epochs, and possibly give me stronger results. Another thing I would like to implement is data augmentation in the image preprocessing. Since the (smaller) size of the dataset is a limiting factor of the model, I would like to apply data augmentation (random rotations, reflections, etc.) on the images to increase the amount of data my model has to train on.
